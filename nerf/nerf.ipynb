{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6135c0",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields\n",
    "\n",
    "This notebook was written while referencing the original NeRF code so as to visualize step by step on how NeRF works. Some functions have been refactored for better understanding but as much as possible, none of the actual code was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc70c7",
   "metadata": {},
   "source": [
    "## Attribution and Citation\n",
    "- Original paper: Mildenhall, Ben; Srinivasan, Pratul P.; Tancik, Matthew; Barron, Jonathan T.; Ramamoorthi, Ravi; Ng, Ren. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” ECCV 2020. [arXiv:2003.08934](https://arxiv.org/abs/2003.08934)\n",
    "- Reference code: [bmild/nerf](https://github.com/bmild/nerf) (MIT License). Copyright © 2020 Ben Mildenhall.\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{mildenhall2020nerf,\n",
    "  title     = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},\n",
    "  author    = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},\n",
    "  booktitle = {European Conference on Computer Vision (ECCV)},\n",
    "  year      = {2020}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc42dfb",
   "metadata": {},
   "source": [
    "## Import Dependencies and Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c328110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third party libraries\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper libraries\n",
    "from render import *\n",
    "from embedder import *\n",
    "from nerf_helpers import *\n",
    "import utils\n",
    "\n",
    "# Loaders\n",
    "from load_llff import *\n",
    "from load_blender import *\n",
    "from load_deepvoxels import * \n",
    "from load_LINEMOD import *\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "DEBUG = False\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82faedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3d9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeted yaml folder: yaml\\llff_fern\n",
      "Loading configuration from yaml\\llff_fern\n",
      "Configuration validation passed! Arguments are valid and correctly set.\n",
      "Experiment name: fern_test\n"
     ]
    }
   ],
   "source": [
    "# Load experiment config from yaml\n",
    "EXPERIMENT_NAME = \"llff_fern\"\n",
    "folder_path = os.path.join(\"yaml\", EXPERIMENT_NAME)\n",
    "print(f\"Targeted yaml folder: {folder_path}\")\n",
    "\n",
    "# Load the yaml data for use later in terms of args.\n",
    "args = utils.load_or_create_config(folder_path)\n",
    "\n",
    "print(f\"Experiment name: {args[\"expname\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfcaacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted args dict to AttrDict (dot-access enabled). Example: args.expname -> fern_test\n"
     ]
    }
   ],
   "source": [
    "# Convert dict-style args to dot-access with recursive wrapping\n",
    "class AttrDict(dict):\n",
    "    \"\"\"Dictionary with attribute-style access that recursively wraps nested dicts/lists.\n",
    "\n",
    "    Example:\n",
    "        d = AttrDict.from_obj({\"a\": {\"b\": 1}, \"c\": [{\"d\": 2}]})\n",
    "        d.a.b == 1\n",
    "        d.c[0].d == 2\n",
    "        d.new_key = 3  # also writes to the underlying dict\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError(f\"No such attribute: {name}\") from e\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "    @classmethod\n",
    "    def from_obj(cls, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return cls({k: cls.from_obj(v) for k, v in obj.items()})\n",
    "        if isinstance(obj, list):\n",
    "            return [cls.from_obj(v) for v in obj]\n",
    "        return obj\n",
    "\n",
    "# If args came from YAML as a plain dict, wrap it for dot access\n",
    "try:\n",
    "    if isinstance(args, dict):\n",
    "        args = AttrDict.from_obj(args)\n",
    "        print(\"Converted args dict to AttrDict (dot-access enabled). Example: args.expname ->\", args.expname)\n",
    "except NameError:\n",
    "    # If this cell runs before args is defined, it's a no-op\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa389515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare 3D sample points for a NeRF-style network by applying positional encodings,\n",
    "run the network on these encodings in memory-safe chunks, and then reshape the results\n",
    "back to the original sampling layout.\n",
    "\n",
    "High-level intuition:\n",
    "- We often sample many 3D points (xyz) per ray and, optionally, use a per-ray viewing\n",
    "  direction. Raw coordinates are hard for small MLPs to learn high-frequency detail,\n",
    "  so we first apply a positional encoding that maps them to a higher-dimensional space.\n",
    "- We flatten everything to a big batch so the network can process all samples uniformly.\n",
    "- To avoid running out of memory, we split this big batch into chunks and process them\n",
    "  sequentially, then stitch the outputs back together and restore the original shape.\n",
    "\"\"\"\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"\n",
    "    Prepare inputs for a NeRF-style MLP and apply the network in chunks.\n",
    "\n",
    "    Conceptual overview:\n",
    "    - Positions (xyz) and, optionally, viewing directions are first positional-encoded\n",
    "      (a deterministic mapping to a higher-dimensional space using sin/cos at multiple\n",
    "      frequencies). This helps the MLP represent fine details and sharp changes.\n",
    "    - We flatten leading dimensions so all samples are processed as a single batch.\n",
    "    - To keep memory usage in check, we process this batch in chunks (netchunk).\n",
    "    - Finally, we reshape outputs to match the original sampling layout.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Sample positions with shape [..., Cpos], typically Cpos = 3.\n",
    "            Example: [N_rays, N_samples, 3]. The leading dimensions can be any shape.\n",
    "        viewdirs (Optional[torch.Tensor]): Per-ray viewing directions with shape\n",
    "            [N_rays, Cdir] (typically Cdir = 3), or None if not using view-dependent effects.\n",
    "            When provided, each ray direction is broadcast to all samples along that ray.\n",
    "        fn (Callable[[torch.Tensor], torch.Tensor]): Neural network (e.g., NeRF MLP) that\n",
    "            consumes encoded features and returns outputs per sample.\n",
    "        embed_fn (Callable[[torch.Tensor], torch.Tensor]): Positional encoder for positions;\n",
    "            maps [*, Cpos] -> [*, Cpos_enc].\n",
    "        embeddirs_fn (Optional[Callable[[torch.Tensor], torch.Tensor]]): Positional encoder\n",
    "            for directions; maps [*, Cdir] -> [*, Cdir_enc]. Only used if viewdirs is not None.\n",
    "        netchunk (int): Maximum number of samples to process per chunk to limit peak memory.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Network outputs with shape [..., Cout], where the leading dimensions\n",
    "        match those of `inputs` (excluding its last channel), and Cout is determined by `fn`.\n",
    "    \"\"\"\n",
    "    # Flatten all leading dimensions so we have a simple [N, Cpos] batch of positions.\n",
    "    # N is the total number of samples across rays and per-ray samples.\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "\n",
    "    # Positional-encode the flattened positions (e.g., apply sin/cos at multiple frequencies).\n",
    "    # This expands each 3D input into a richer, higher-dimensional representation\n",
    "    # that makes it easier for the MLP to model fine spatial detail.\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    # If using view-dependent appearance (e.g., specular highlights that vary with direction),\n",
    "    # we also encode per-ray viewing directions and concatenate them with position encodings.\n",
    "    if viewdirs is not None:\n",
    "        # Insert a length-1 axis, then broadcast each ray direction across all samples on that ray\n",
    "        # so that every sample point along a ray shares the same view direction.\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape)\n",
    "\n",
    "        # Flatten directions to align with the flattened positions: [N, Cdir].\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "\n",
    "        # Positional-encode viewing directions in the same spirit as positions.\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "\n",
    "        # Concatenate encoded positions and encoded directions along the feature/channel axis.\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    # Apply the network to the encoded features in memory-safe chunks along the batch dimension.\n",
    "    # This prevents out-of-memory errors when the total number of samples is very large.\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "\n",
    "    # Restore the original leading shape (e.g., [N_rays, N_samples]) and append the output channels.\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf79950",
   "metadata": {},
   "source": [
    "## Instantiate NeRF\n",
    "This section creates a function to instantiate NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cab3b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf import NeRF\n",
    "\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\n",
    "    \"\"\"\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    model = NeRF(D=args.netdepth, W=args.netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                netchunk=args.netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce81b35",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f36d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    End-to-end NeRF training loop.\n",
    "\n",
    "    High-level overview for newcomers:\n",
    "    - Load a dataset of posed images (e.g., LLFF/Blender/LINEMOD). Each image comes with a camera pose.\n",
    "    - Create a NeRF model (coarse and optionally fine MLPs) and a renderer.\n",
    "    - On each iteration, sample camera rays and their target RGB values from the dataset.\n",
    "    - Render rays with the NeRF model via volumetric rendering (accumulate colors along the ray).\n",
    "    - Compute a reconstruction loss (e.g., MSE) against ground-truth pixels and optimize the networks.\n",
    "    - Periodically render validation trajectories and/or save snapshots.\n",
    "\n",
    "    Key concepts:\n",
    "    - Rays: For each pixel, we cast a ray into the scene with origin/direction computed from intrinsics and pose.\n",
    "    - Sampling: We sample multiple points along each ray (coarse). Optionally resample (fine) where the scene is likely.\n",
    "    - Volume rendering: Convert per-point density+color to opacity weights and composite to a final pixel color.\n",
    "    - Hierarchical sampling: A second pass focuses samples where the coarse pass is confident the scene exists.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------\n",
    "    # 1) Load data and choose near/far bounds depending on dataset\n",
    "    # ----------------------\n",
    "    K = None\n",
    "    if args.dataset_type == 'llff':\n",
    "        images, poses, bds, render_poses, i_test = load_llff_data(\n",
    "            args.datadir, args.factor, recenter=True, bd_factor=.75, spherify=args.spherify\n",
    "        )\n",
    "        hwf = poses[0,:3,-1]           # (H, W, focal)\n",
    "        poses = poses[:,:3,:4]         # Only keep rotation+translation (3x4) per pose\n",
    "        print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        if not isinstance(i_test, list):\n",
    "            i_test = [i_test]\n",
    "\n",
    "        # Optional LLFF holdout: use every N-th image as test\n",
    "        if args.llffhold > 0:\n",
    "            print('Auto LLFF holdout,', args.llffhold)\n",
    "            i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "\n",
    "        i_val = i_test\n",
    "        i_train = np.array([i for i in np.arange(int(images.shape[0]))\n",
    "                            if (i not in i_test and i not in i_val)])\n",
    "\n",
    "        print('DEFINING BOUNDS')\n",
    "        if args.no_ndc:\n",
    "            # If not using NDC (e.g., inward-facing/360 scenes), near/far from bounds\n",
    "            near = np.ndarray.min(bds) * .9\n",
    "            far = np.ndarray.max(bds) * 1.\n",
    "        else:\n",
    "            # Forward-facing (LLFF) uses NDC, so near/far are normalized\n",
    "            near = 0.\n",
    "            far = 1.\n",
    "        print('NEAR FAR', near, far)\n",
    "\n",
    "    elif args.dataset_type == 'blender':\n",
    "        images, poses, render_poses, hwf, i_split = load_blender_data(\n",
    "            args.datadir, args.half_res, args.testskip\n",
    "        )\n",
    "        print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        # Standard near/far for Blender synthetic scenes\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        # Composite over white if requested (makes background white instead of black)\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "    elif args.dataset_type == 'LINEMOD':\n",
    "        images, poses, render_poses, hwf, K, i_split, near, far = load_LINEMOD_data(\n",
    "            args.datadir, args.half_res, args.testskip\n",
    "        )\n",
    "        print(f'Loaded LINEMOD, images shape: {images.shape}, hwf: {hwf}, K: {K}')\n",
    "        print(f'[CHECK HERE] near: {near}, far: {far}.')\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "    elif args.dataset_type == 'deepvoxels':\n",
    "        images, poses, render_poses, hwf, i_split = load_dv_data(\n",
    "            scene=args.shape, basedir=args.datadir, testskip=args.testskip\n",
    "        )\n",
    "        print('Loaded deepvoxels', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        # DeepVoxels scenes define a hemisphere radius; near/far around it\n",
    "        hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))\n",
    "        near = hemi_R-1.\n",
    "        far = hemi_R+1.\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    # ----------------------\n",
    "    # 2) Prepare intrinsics (H, W, focal) and default K if not provided\n",
    "    # ----------------------\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if K is None:\n",
    "        # Construct a pinhole intrinsics matrix assuming principal point at image center\n",
    "        K = np.array([\n",
    "            [focal, 0, 0.5*W],\n",
    "            [0, focal, 0.5*H],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    # If we are evaluating on the test set, use the corresponding subset of poses\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    # ----------------------\n",
    "    # 3) Logging setup and persistence of configs\n",
    "    # ----------------------\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "\n",
    "    # Save the parsed args for reproducibility\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg, attr in sorted(args.items()):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    # Save the YAML configuration that produced these args\n",
    "    f_yaml = os.path.join(basedir, expname, 'config.yaml')\n",
    "    try:\n",
    "        utils.save_yaml(dict(args), f_yaml)\n",
    "    except Exception:\n",
    "        # Fallback: write a simple YAML dump directly\n",
    "        with open(f_yaml, 'w', encoding='utf-8') as yf:\n",
    "            import yaml as _yaml\n",
    "            _yaml.dump(dict(args), yf, default_flow_style=False, indent=2)\n",
    "\n",
    "    # ----------------------\n",
    "    # 4) Create NeRF models and optimizer\n",
    "    # ----------------------\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n",
    "    global_step = start\n",
    "\n",
    "    # Near/far bounds are used by the renderer; update both train and test configs\n",
    "    bds_dict = { 'near' : near, 'far' : far }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move the camera trajectory used for rendering validation videos to the GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "\n",
    "    # ----------------------\n",
    "    # 5) Short-circuit: render only mode\n",
    "    # ----------------------\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if args.render_test:\n",
    "                # Switch to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(\n",
    "                basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start)\n",
    "            )\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "\n",
    "            # Render a path and save to video\n",
    "            rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test,\n",
    "                                   gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "            print('Done rendering', testsavedir)\n",
    "            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "            return\n",
    "\n",
    "    # ----------------------\n",
    "    # 6) Prepare random-ray batching (optional) and move data to GPU\n",
    "    # ----------------------\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        # Precompute all rays for all training images, then shuffle mini-batches each step\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0)  # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1)                   # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4])                         # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0)                 # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3])                              # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move arrays/tensors to GPU for training\n",
    "    if use_batching:\n",
    "        images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)\n",
    "\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    # ----------------------\n",
    "    # 7) Main optimization loop\n",
    "    # ----------------------\n",
    "    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))  # Optional TB logging\n",
    "\n",
    "    start = start + 1\n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time()\n",
    "\n",
    "        # Sample a batch of rays and target colors\n",
    "        if use_batching:\n",
    "            # Random over all images (global batching)\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand]  # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            # Move sliding window; reshuffle at epoch end\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "        else:\n",
    "            # Random rays from a randomly chosen training image (per-image batching)\n",
    "            img_i = np.random.choice(i_train)\n",
    "            target = images[img_i]\n",
    "            target = torch.Tensor(target).to(device)\n",
    "            pose = poses[img_i, :3,:4]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                # Compute per-pixel rays for this image, then sample N_rand of them\n",
    "                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n",
    "\n",
    "                if i < args.precrop_iters:\n",
    "                    # Optional: focus early training on the image center (stabilizes training)\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n",
    "                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n",
    "                        ), -1)\n",
    "                    if i == start:\n",
    "                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n",
    "                else:\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1\n",
    "                    )  # (H, W, 2)\n",
    "\n",
    "                coords = torch.reshape(coords, [-1,2])                         # (H*W, 2)\n",
    "                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n",
    "                select_coords = coords[select_inds].long()                     # (N_rand, 2)\n",
    "                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]      # (N_rand, 3)\n",
    "                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]      # (N_rand, 3)\n",
    "                batch_rays = torch.stack([rays_o, rays_d], 0)\n",
    "                target_s = target[select_coords[:, 0], select_coords[:, 1]]    # (N_rand, 3)\n",
    "\n",
    "        # ---- Core rendering + loss ----\n",
    "        rgb, disp, acc, extras = render(\n",
    "            H, W, K, chunk=args.chunk, rays=batch_rays, verbose=i < 10, retraw=True, **render_kwargs_train\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_loss = img2mse(rgb, target_s)\n",
    "        trans = extras['raw'][...,-1]\n",
    "        loss = img_loss\n",
    "        psnr = mse2psnr(img_loss)\n",
    "\n",
    "        # If hierarchical sampling is enabled, include the coarse-pass loss\n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Learning rate decay (exponential) ---\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = args.lrate_decay * 1000\n",
    "        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "\n",
    "        dt = time.time()-time0\n",
    "        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 8) Periodic logging, checkpointing, and visualization\n",
    "        # ----------------------\n",
    "        if i%args.i_weights==0:\n",
    "            # Save model checkpoints for resuming or analysis\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            torch.save({\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i%args.i_video==0 and i > 0:\n",
    "            # Render a validation trajectory and write MP4 previews\n",
    "            with torch.no_grad():\n",
    "                rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            # If you want to visualize view-dependent effects, you can fix the camera position\n",
    "            # and vary only the view direction (see commented example in original code).\n",
    "\n",
    "        if i%args.i_testset==0 and i > 0:\n",
    "            # Render the held-out test set and save frames\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk,\n",
    "                            render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "        if i%args.i_print==0:\n",
    "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "            # Additional TensorBoard logging code is kept in comments in the original.\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c482509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert training parameters\n",
    "N_iters = 500 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5f87ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image data (378, 504, 3, 20) [378.         504.         407.56579161]\n",
      "Loaded ./data/nerf_llff_data/fern 16.985296178676084 80.00209740336334\n",
      "recentered (3, 5)\n",
      "[[ 1.0000000e+00  0.0000000e+00  0.0000000e+00 -2.9802323e-09]\n",
      " [ 0.0000000e+00  1.0000000e+00 -1.8730975e-09 -9.6857544e-09]\n",
      " [-0.0000000e+00  1.8730975e-09  1.0000000e+00 -7.4505807e-10]]\n",
      "Data:\n",
      "(20, 3, 5) (20, 378, 504, 3) (20, 2)\n",
      "HOLDOUT view is 12\n",
      "Loaded llff (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.0 1.0\n",
      "Found ckpts []\n",
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n",
      "Begin\n",
      "TRAIN views are [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "TEST views are [ 0  8 16]\n",
      "VAL views are [ 0  8 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [00:09<00:36, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 100 Loss: 0.0506649985909462  PSNR: 15.991083145141602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [00:18<00:27, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 200 Loss: 0.04145825281739235  PSNR: 16.938528060913086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 301/500 [00:27<00:18, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 300 Loss: 0.03398731350898743  PSNR: 17.717384338378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [00:36<00:08, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 400 Loss: 0.02866814285516739  PSNR: 18.31490707397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:45<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 500 Loss: 0.0285334549844265  PSNR: 18.495981216430664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c989e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
