{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6135c0",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields\n",
    "\n",
    "This notebook was written while referencing the original NeRF code so as to visualize step by step on how NeRF works. Some functions have been refactored for better understanding but as much as possible, none of the actual code was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc70c7",
   "metadata": {},
   "source": [
    "## Attribution and Citation\n",
    "- Original paper: Mildenhall, Ben; Srinivasan, Pratul P.; Tancik, Matthew; Barron, Jonathan T.; Ramamoorthi, Ravi; Ng, Ren. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” ECCV 2020. [arXiv:2003.08934](https://arxiv.org/abs/2003.08934)\n",
    "- Reference code: [bmild/nerf](https://github.com/bmild/nerf) (MIT License). Copyright © 2020 Ben Mildenhall.\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{mildenhall2020nerf,\n",
    "  title     = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},\n",
    "  author    = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},\n",
    "  booktitle = {European Conference on Computer Vision (ECCV)},\n",
    "  year      = {2020}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc42dfb",
   "metadata": {},
   "source": [
    "## Import Dependencies and Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c328110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Third party libraries\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper libraries\n",
    "from embedder import *\n",
    "from nerf_helpers import *\n",
    "import utils\n",
    "\n",
    "# Loaders\n",
    "from load_llff import *\n",
    "from load_blender import *\n",
    "from load_deepvoxels import * \n",
    "from load_LINEMOD import *\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "DEBUG = False\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82faedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3d9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeted yaml folder: yaml\\llff_fern\n",
      "Loading configuration from yaml\\llff_fern\n",
      "Configuration validation passed! Arguments are valid and correctly set.\n",
      "Experiment name: fern_test\n"
     ]
    }
   ],
   "source": [
    "# Load experiment config from yaml\n",
    "EXPERIMENT_NAME = \"llff_fern\"\n",
    "folder_path = os.path.join(\"yaml\", EXPERIMENT_NAME)\n",
    "print(f\"Targeted yaml folder: {folder_path}\")\n",
    "\n",
    "# Load the yaml data for use later in terms of args.\n",
    "args = utils.load_or_create_config(folder_path)\n",
    "\n",
    "print(f\"Experiment name: {args[\"expname\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfcaacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted args dict to AttrDict (dot-access enabled). Example: args.expname -> fern_test\n"
     ]
    }
   ],
   "source": [
    "# Convert dict-style args to dot-access with recursive wrapping\n",
    "class AttrDict(dict):\n",
    "    \"\"\"Dictionary with attribute-style access that recursively wraps nested dicts/lists.\n",
    "\n",
    "    Example:\n",
    "        d = AttrDict.from_obj({\"a\": {\"b\": 1}, \"c\": [{\"d\": 2}]})\n",
    "        d.a.b == 1\n",
    "        d.c[0].d == 2\n",
    "        d.new_key = 3  # also writes to the underlying dict\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError(f\"No such attribute: {name}\") from e\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "    @classmethod\n",
    "    def from_obj(cls, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return cls({k: cls.from_obj(v) for k, v in obj.items()})\n",
    "        if isinstance(obj, list):\n",
    "            return [cls.from_obj(v) for v in obj]\n",
    "        return obj\n",
    "\n",
    "# If args came from YAML as a plain dict, wrap it for dot access\n",
    "try:\n",
    "    if isinstance(args, dict):\n",
    "        args = AttrDict.from_obj(args)\n",
    "        print(\"Converted args dict to AttrDict (dot-access enabled). Example: args.expname ->\", args.expname)\n",
    "except NameError:\n",
    "    # If this cell runs before args is defined, it's a no-op\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5538f3",
   "metadata": {},
   "source": [
    "## Render\n",
    "\n",
    "NeRF rendering utilities\n",
    "\n",
    "This part contains helper functions to render volumes using Neural Radiance Fields (NeRF).\n",
    "If you're new to NeRF, here's the high-level idea you'll see reflected in the code:\n",
    "\n",
    "- A NeRF model takes a 3D point (and often a viewing direction) and predicts color (RGB)\n",
    "  and density (sigma) at that point.\n",
    "- To render an image, we cast a ray through each pixel, sample many points along the ray,\n",
    "  evaluate the network at those points, and composite the colors using volume rendering.\n",
    "- We optionally do this in two passes: a coarse pass to find where the scene is, then a\n",
    "  fine pass that samples more densely in the important regions (importance sampling).\n",
    "\n",
    "The functions below help with:\n",
    "- Splitting big tensors/ray sets into smaller chunks to avoid out-of-memory issues.\n",
    "- Converting raw network outputs into rendered RGB/depth/opacity via volume rendering.\n",
    "- Orchestrating the full per-ray rendering with optional hierarchical sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d11a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following function wraps a function fn so it processes a large input tensor in smaller chunks\n",
    "to reduce peak memory usage, then stitches the result together.\n",
    "\n",
    "This splits inputs along the first dimension into slices of size chunk and applies fn to each slice.\n",
    "Then it concatenates the results back together along the first dimension.\n",
    "\n",
    "This reduces gpu and cpu spikes when fn is heavy (i.e neural network forward pass) and preserves\n",
    "autograd, as gradients flow through torch.cat and slicing.\n",
    "\"\"\"\n",
    "def batchify(fn, chunk):\n",
    "    \"\"\"Wrap a function so it runs on smaller input chunks to reduce peak memory.\n",
    "\n",
    "    This is useful for heavy functions like neural network forward passes. Instead of\n",
    "    evaluating the entire input tensor in one go, we split it along the first dimension\n",
    "    into slices of size ``chunk`` and then concatenate the results.\n",
    "\n",
    "    Args:\n",
    "        fn (Callable[[Tensor], Tensor]): The function to run on chunks.\n",
    "        chunk (Optional[int]): Number of rows to process per call. If ``None``,\n",
    "            no chunking is performed.\n",
    "\n",
    "    Returns:\n",
    "        Callable[[Tensor], Tensor]: A wrapper that applies ``fn`` on input chunks and\n",
    "        stitches the outputs along the first dimension.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c35c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n",
    "    \"\"\"\n",
    "    Convert raw network predictions along rays into rendered outputs using\n",
    "    volumetric rendering (accumulation over samples).\n",
    "\n",
    "    High-level intuition (NeRF volume rendering):\n",
    "    - The network predicts, per sampled point along a ray, an RGB value and a density\n",
    "      (often called sigma). Density indicates how much light is absorbed/emitted there.\n",
    "    - We turn densities into per-sample opacities (alpha) based on the distance between\n",
    "      adjacent samples along the ray.\n",
    "    - We compute transmittance (how much light makes it to a sample without being blocked)\n",
    "      via a cumulative product, then form per-sample weights = transmittance * alpha.\n",
    "    - We composite colors, depths, and other quantities by weighted sums over samples.\n",
    "\n",
    "    Args:\n",
    "        raw (torch.Tensor): [N_rays, N_samples, 4] raw predictions per sample. The first\n",
    "            3 channels are RGB logits (before sigmoid), and the last channel is density (sigma).\n",
    "        z_vals (torch.Tensor): [N_rays, N_samples] sample depths or t-values along each ray.\n",
    "        rays_d (torch.Tensor): [N_rays, 3] direction vectors for each ray. Used to scale\n",
    "            step sizes from parametric units to metric distances.\n",
    "        raw_noise_std (float): Stddev of Gaussian noise added to sigma during training for\n",
    "            regularization. Set to 0.0 at eval time.\n",
    "        white_bkgd (bool): If True, composite the result over a white background (useful\n",
    "            for synthetic datasets rendered on white).\n",
    "        pytest (bool): If True, use deterministic numpy noise for reproducible tests.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - rgb_map (torch.Tensor): [N_rays, 3] rendered RGB color per ray.\n",
    "            - disp_map (torch.Tensor): [N_rays] disparity (inverse depth) per ray.\n",
    "            - acc_map (torch.Tensor): [N_rays] accumulated opacity per ray (sum of weights).\n",
    "            - weights (torch.Tensor): [N_rays, N_samples] per-sample contribution weights.\n",
    "            - depth_map (torch.Tensor): [N_rays] expected depth per ray.\n",
    "    \"\"\"\n",
    "    # Map density (sigma) and step size (distance between samples) to opacity (alpha):\n",
    "    #   alpha = 1 - exp(-relu(sigma) * delta)\n",
    "    # relu ensures sigma is non-negative, as negative density is not physical.\n",
    "    raw2alpha = lambda raw_sigma, dists, act_fn=F.relu: 1.0 - torch.exp(-act_fn(raw_sigma) * dists)\n",
    "\n",
    "    # Compute distances between adjacent samples along each ray in z (or t) space.\n",
    "    # Shape after diff: [N_rays, N_samples-1]\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\n",
    "    # For the last sample on each ray, append a very large distance so that its\n",
    "    # contribution is properly modeled as the ray exiting the volume.\n",
    "    # Resulting shape: [N_rays, N_samples]\n",
    "    dists = torch.cat([dists, torch.tensor([1e10], device=z_vals.device, dtype=z_vals.dtype).expand(dists[..., :1].shape)], dim=-1)\n",
    "\n",
    "    # Convert parametric distances to metric distances by multiplying by the ray length.\n",
    "    # This accounts for non-unit ray directions. rays_d[..., None, :] has shape [N_rays, 1, 3]\n",
    "    # and we take its L2 norm to scale each sample distance.\n",
    "    dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "    # Convert raw RGB logits to [0,1] colors per sample.\n",
    "    rgb = torch.sigmoid(raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "\n",
    "    # Optional noise added to densities during training for regularization.\n",
    "    # Ensure raw_noise_std is a float (configs may pass strings).\n",
    "    try:\n",
    "        noise_std = float(raw_noise_std)\n",
    "    except (TypeError, ValueError):\n",
    "        noise_std = 0.0\n",
    "\n",
    "    noise = 0.0\n",
    "    if noise_std > 0.0:\n",
    "        noise = torch.randn(raw[..., 3].shape, device=raw.device, dtype=raw.dtype) * noise_std\n",
    "\n",
    "        # Deterministic noise path for unit tests.\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            noise_np = np.random.rand(*list(raw[..., 3].shape)) * noise_std\n",
    "            noise = torch.tensor(noise_np, device=raw.device, dtype=raw.dtype)\n",
    "\n",
    "    # Opacity per sample from density and distance.\n",
    "    alpha = raw2alpha(raw[..., 3] + noise, dists)  # [N_rays, N_samples]\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=alpha.device, dtype=alpha.dtype), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "\n",
    "    # Rendered color is the weighted sum of per-sample colors along the ray.\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [N_rays, 3]\n",
    "\n",
    "    # Expected depth is the weighted sum of sample depths.\n",
    "    depth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "    # Disparity is inverse depth. We divide expected depth by total weight (visibility)\n",
    "    # and guard with epsilon to avoid divide-by-zero when the ray hits nothing.\n",
    "    denom = torch.max(1e-10 * torch.ones_like(depth_map), torch.sum(weights, dim=-1))\n",
    "    disp_map = 1.0 / torch.clamp(depth_map / denom, min=1e-10)\n",
    "\n",
    "    # Accumulated opacity along the ray (how much of the ray got \"stopped\").\n",
    "    acc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "    # If the scene assumes a white background, composite the missing transmittance as white.\n",
    "    if white_bkgd:\n",
    "        rgb_map = rgb_map + (1.0 - acc_map[..., None])\n",
    "\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6208cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False):\n",
    "    \"\"\"Render a bundle of rays using NeRF-style volume rendering.\n",
    "\n",
    "    High-level steps for each ray:\n",
    "    1) Sample N points between near and far along the ray (evenly in depth or inverse depth).\n",
    "    2) Query the network at those 3D points (and optionally view directions) to get raw RGB+sigma.\n",
    "    3) Convert raw predictions to colors via volume rendering (accumulate with alphas/weights).\n",
    "    4) If enabled, run hierarchical (importance) sampling: take a second set of samples drawn\n",
    "       from a PDF defined by the coarse weights, re-evaluate the network (fine), and re-render.\n",
    "\n",
    "    Args:\n",
    "        ray_batch (torch.Tensor): [num_rays, Cray]. Per-ray data packed together. The first\n",
    "            3 entries are ray origins, next 3 are ray directions, next 2 are near/far bounds,\n",
    "            and the last 3 (if present) are unit view directions for view-dependent effects.\n",
    "        network_fn (Callable): The coarse NeRF MLP. Given points (and viewdirs), predicts\n",
    "            raw RGB (logits) and density (sigma).\n",
    "        network_query_fn (Callable): A helper that formats inputs and calls the network.\n",
    "        N_samples (int): Number of stratified samples for the coarse pass.\n",
    "        retraw (bool): If True, also return the raw outputs from the last pass.\n",
    "        lindisp (bool): If True, sample uniformly in inverse depth (disparity) instead of depth.\n",
    "            This concentrates samples near the camera, helpful for scenes with large depth ranges.\n",
    "        perturb (float): If > 0, enable stratified sampling noise during training for anti-aliasing.\n",
    "        N_importance (int): Extra samples for the fine pass (hierarchical sampling). 0 disables it.\n",
    "        network_fine (Optional[Callable]): A separate fine MLP. If None, reuse ``network_fn``.\n",
    "        white_bkgd (bool): If True, composite the result over a white background.\n",
    "        raw_noise_std (float): Stddev of Gaussian noise added to density during training.\n",
    "        verbose (bool): If True, print additional debug information.\n",
    "        pytest (bool): If True, make randomness deterministic for unit tests.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: Always includes:\n",
    "            - ``rgb_map`` [num_rays, 3]: Rendered color from the last pass (fine if enabled).\n",
    "            - ``disp_map`` [num_rays]: Disparity (1/depth) from the last pass.\n",
    "            - ``acc_map`` [num_rays]: Accumulated opacity from the last pass.\n",
    "        Optionally includes:\n",
    "            - ``raw`` [num_rays, num_samples, 4]: Raw outputs of the last pass if ``retraw``.\n",
    "            - ``rgb0``, ``disp0``, ``acc0``: Coarse pass results when ``N_importance > 0``.\n",
    "            - ``z_std`` [num_rays]: Std. dev. of fine samples (measures sampling concentration).\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    # Unpack packed ray data: origins, directions, near/far bounds, and optional viewdirs.\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "\n",
    "    # Step 1: Choose parametric sample positions t in [0, 1] and map to depths z in [near, far].\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples, device=ray_batch.device)\n",
    "    if not lindisp:\n",
    "        # Uniform samples in depth\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        # Uniform samples in inverse depth (places more samples closer to the camera)\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # During training, jitter samples within each interval for stratified sampling.\n",
    "        # This reduces aliasing and improves robustness.\n",
    "        # Get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # Draw stratified samples inside those intervals\n",
    "        t_rand = torch.rand(z_vals.shape, device=z_vals.device)\n",
    "\n",
    "        # Pytest, overwrite u with numpy's fixed random numbers\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.tensor(t_rand, device=z_vals.device, dtype=z_vals.dtype)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # Compute 3D sample locations along each ray: o + t*d for each sampled depth t (z_vals).\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "\n",
    "    # raw = run_network(pts)\n",
    "    # Query the (coarse) network at all sample points. ``viewdirs`` enables view-dependent effects.\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0:\n",
    "\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
    "\n",
    "        # Hierarchical sampling (importance sampling):\n",
    "        # Build a PDF from coarse weights and draw additional samples where the scene is likely.\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        # Exclude the first and last weights to avoid boundary artifacts when forming the PDF.\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n",
    "        # Detach so gradients do not flow through the sampling operation.\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        # Merge coarse and fine samples, then sort along the ray.\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        # Use dedicated fine network if provided, else reuse the coarse network.\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "        # raw = run_network(pts, fn=run_fn)\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        # Include coarse pass results for potential losses or visualization.\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        # Standard deviation of fine samples: indicates how concentrated sampling is per ray.\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30896c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Render a large set of rays by splitting them into manageable minibatches to avoid\n",
    "out-of-memory (OOM) issues, then stitch the per-batch results back together.\n",
    "\n",
    "High-level intuition:\n",
    "- Rendering involves evaluating many rays (often millions). Each ray requires sampling\n",
    "  points, running an MLP, and compositing colors/densities, which can be memory-heavy.\n",
    "- Instead of rendering all rays at once, we process them in chunks (minibatches), which\n",
    "  keeps peak memory usage bounded.\n",
    "- We collect and concatenate the results for each output quantity (e.g., rgb, depth, acc).\n",
    "\"\"\"\n",
    "\n",
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"\n",
    "    Render rays in smaller minibatches to avoid OOM.\n",
    "\n",
    "    Args:\n",
    "        rays_flat (torch.Tensor): Rays flattened along the batch dimension, shape [N, Cray].\n",
    "            Each row encodes a single ray's data (e.g., origin, direction, near/far, etc.).\n",
    "        chunk (int): Number of rays to render per minibatch.\n",
    "        **kwargs: Additional keyword arguments forwarded to `render_rays` (e.g., models,\n",
    "            sampling counts, noise parameters).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: A dictionary where each key corresponds to a rendered\n",
    "        quantity (e.g., 'rgb_map', 'disp_map', 'acc_map', etc.), and each tensor has\n",
    "        shape [N, ...], formed by concatenating per-chunk outputs along the first dimension.\n",
    "    \"\"\"\n",
    "    # Accumulate lists of per-chunk outputs in a dictionary keyed by output name.\n",
    "    all_ret = {}\n",
    "\n",
    "    # Iterate over rays in chunks: [0:chunk], [chunk:2*chunk], ...\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        # Render a minibatch of rays using the provided rendering function and settings.\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "\n",
    "        # For each output field produced by the renderer, append the minibatch result\n",
    "        # to a growing list so we can concatenate later.\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    # Concatenate lists of chunk results into full [N, ...] tensors for each field.\n",
    "    all_ret = {k: torch.cat(all_ret[k], dim=0) for k in all_ret}\n",
    "\n",
    "    return all_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e728037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True,\n",
    "                  near=0., far=1.,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render a full image or a provided set of rays using NeRF.\n",
    "\n",
    "    There are two common ways to call this function:\n",
    "    - Full image: pass a camera-to-world matrix ``c2w`` and camera intrinsics ``K``.\n",
    "      The function will generate one ray per pixel and render all of them.\n",
    "    - Custom rays: pass ``rays=(rays_o, rays_d)`` to render only those rays.\n",
    "\n",
    "    Args:\n",
    "        H (int): Image height in pixels.\n",
    "        W (int): Image width in pixels.\n",
    "        K (torch.Tensor or np.ndarray): 3x3 camera intrinsics matrix. We use ``K[0,0]``\n",
    "            (focal length in pixels) when converting to NDC for forward-facing scenes.\n",
    "        chunk (int): Max number of rays to process per minibatch to control memory usage.\n",
    "        rays (tuple[Tensor, Tensor], optional): Tuple ``(rays_o, rays_d)`` with shapes\n",
    "            [..., 3] each, giving ray origins and directions. If provided, ``c2w`` is ignored.\n",
    "        c2w (torch.Tensor or np.ndarray, optional): [3,4] camera-to-world matrix. If provided,\n",
    "            rays for the full image are generated via ``get_rays``.\n",
    "        ndc (bool): If True, convert rays to normalized device coordinates (recommended for\n",
    "            forward-facing scenes as in the original NeRF paper).\n",
    "        near (float): Near plane distance used to initialize per-ray near bounds.\n",
    "        far (float): Far plane distance used to initialize per-ray far bounds.\n",
    "        use_viewdirs (bool): If True, pass unit viewing directions to the network to enable\n",
    "            view-dependent appearance (specularities).\n",
    "        c2w_staticcam (torch.Tensor or np.ndarray, optional): If provided with ``use_viewdirs``\n",
    "            enabled, generate rays from ``c2w_staticcam`` but keep view directions from ``c2w``.\n",
    "            This is useful to visualize how view-dependent effects change with direction.\n",
    "        **kwargs: Forwarded to ``render_rays`` (e.g., networks, sample counts, noise settings).\n",
    "\n",
    "    Returns:\n",
    "        list: ``[rgb_map, disp_map, acc_map, extras]``\n",
    "            - rgb_map: [H, W, 3] rendered colors\n",
    "            - disp_map: [H, W] disparity (1/depth)\n",
    "            - acc_map: [H, W] accumulated opacity\n",
    "            - extras: dict with any additional outputs from ``render_rays``\n",
    "    \"\"\"\n",
    "    # Infer rendering device from the model to keep all tensors consistent\n",
    "    model_device = next(kwargs['network_fn'].parameters()).device\n",
    "\n",
    "    if c2w is not None:\n",
    "        # Special case: render a full image by generating one ray per pixel.\n",
    "        rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "    else:\n",
    "        # Use the provided custom ray batch.\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # Provide normalized ray directions to the network for view-dependent effects.\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # Visualize only the effect of changing view direction while keeping camera fixed.\n",
    "            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float().to(model_device)\n",
    "\n",
    "    sh = rays_d.shape # [..., 3]\n",
    "    if ndc:\n",
    "        # Convert to NDC (assumes a pinhole camera model), commonly used for LLFF/forward-facing scenes.\n",
    "        rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float().to(model_device)\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float().to(model_device)\n",
    "\n",
    "    # Initialize per-ray near/far bounds and pack rays into a single tensor expected by render_rays.\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "\n",
    "    # Render all rays in memory-friendly chunks, then reshape results back to image grids.\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f0ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "    \"\"\"Render a sequence of camera poses to produce a video or trajectory.\n",
    "\n",
    "    This convenience function loops over a list/array of camera-to-world matrices and calls\n",
    "    ``render`` for each pose. Optionally writes frames to ``savedir`` and/or renders at a\n",
    "    lower resolution for speed.\n",
    "\n",
    "    Args:\n",
    "        render_poses (Iterable[Tensor or np.ndarray]): Sequence of [3,4] camera-to-world matrices.\n",
    "        hwf (tuple): ``(H, W, focal)`` from dataset metadata. Only ``H`` and ``W`` are used here.\n",
    "        K (Tensor or np.ndarray): 3x3 intrinsics matrix passed through to ``render``.\n",
    "        chunk (int): Chunk size forwarded to ``render``.\n",
    "        render_kwargs (dict): Keyword args forwarded to ``render`` (e.g., networks and settings).\n",
    "        gt_imgs (optional): Ground-truth images; if provided, you can compute metrics (example below).\n",
    "        savedir (str, optional): If provided, write each rendered RGB frame as a PNG to this folder.\n",
    "        render_factor (int): If > 0, downsample H and W by this factor to render faster.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: ``(rgbs, disps)`` where both are stacks over time with shapes\n",
    "        [N, H, W, 3] and [N, H, W] respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed by reducing both resolution and focal length proportionally.\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        # Simple timing print to monitor rendering speed\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "\n",
    "        # Render the current pose; we discard the accumulated opacity and extras here\n",
    "        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        # Example: compute PSNR vs. ground truth if you have it (kept commented by default)\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        # Optionally write the frame to disk as an 8-bit PNG.\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    # Stack lists into contiguous arrays with a time dimension.\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46feb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0111889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare 3D sample points for a NeRF-style network by applying positional encodings,\n",
    "run the network on these encodings in memory-safe chunks, and then reshape the results\n",
    "back to the original sampling layout.\n",
    "\n",
    "High-level intuition:\n",
    "- We often sample many 3D points (xyz) per ray and, optionally, use a per-ray viewing\n",
    "  direction. Raw coordinates are hard for small MLPs to learn high-frequency detail,\n",
    "  so we first apply a positional encoding that maps them to a higher-dimensional space.\n",
    "- We flatten everything to a big batch so the network can process all samples uniformly.\n",
    "- To avoid running out of memory, we split this big batch into chunks and process them\n",
    "  sequentially, then stitch the outputs back together and restore the original shape.\n",
    "\"\"\"\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"\n",
    "    Prepare inputs for a NeRF-style MLP and apply the network in chunks.\n",
    "\n",
    "    Conceptual overview:\n",
    "    - Positions (xyz) and, optionally, viewing directions are first positional-encoded\n",
    "      (a deterministic mapping to a higher-dimensional space using sin/cos at multiple\n",
    "      frequencies). This helps the MLP represent fine details and sharp changes.\n",
    "    - We flatten leading dimensions so all samples are processed as a single batch.\n",
    "    - To keep memory usage in check, we process this batch in chunks (netchunk).\n",
    "    - Finally, we reshape outputs to match the original sampling layout.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Sample positions with shape [..., Cpos], typically Cpos = 3.\n",
    "            Example: [N_rays, N_samples, 3]. The leading dimensions can be any shape.\n",
    "        viewdirs (Optional[torch.Tensor]): Per-ray viewing directions with shape\n",
    "            [N_rays, Cdir] (typically Cdir = 3), or None if not using view-dependent effects.\n",
    "            When provided, each ray direction is broadcast to all samples along that ray.\n",
    "        fn (Callable[[torch.Tensor], torch.Tensor]): Neural network (e.g., NeRF MLP) that\n",
    "            consumes encoded features and returns outputs per sample.\n",
    "        embed_fn (Callable[[torch.Tensor], torch.Tensor]): Positional encoder for positions;\n",
    "            maps [*, Cpos] -> [*, Cpos_enc].\n",
    "        embeddirs_fn (Optional[Callable[[torch.Tensor], torch.Tensor]]): Positional encoder\n",
    "            for directions; maps [*, Cdir] -> [*, Cdir_enc]. Only used if viewdirs is not None.\n",
    "        netchunk (int): Maximum number of samples to process per chunk to limit peak memory.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Network outputs with shape [..., Cout], where the leading dimensions\n",
    "        match those of `inputs` (excluding its last channel), and Cout is determined by `fn`.\n",
    "    \"\"\"\n",
    "    # Flatten all leading dimensions so we have a simple [N, Cpos] batch of positions.\n",
    "    # N is the total number of samples across rays and per-ray samples.\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "\n",
    "    # Positional-encode the flattened positions (e.g., apply sin/cos at multiple frequencies).\n",
    "    # This expands each 3D input into a richer, higher-dimensional representation\n",
    "    # that makes it easier for the MLP to model fine spatial detail.\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    # If using view-dependent appearance (e.g., specular highlights that vary with direction),\n",
    "    # we also encode per-ray viewing directions and concatenate them with position encodings.\n",
    "    if viewdirs is not None:\n",
    "        # Insert a length-1 axis, then broadcast each ray direction across all samples on that ray\n",
    "        # so that every sample point along a ray shares the same view direction.\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape)\n",
    "\n",
    "        # Flatten directions to align with the flattened positions: [N, Cdir].\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "\n",
    "        # Positional-encode viewing directions in the same spirit as positions.\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "\n",
    "        # Concatenate encoded positions and encoded directions along the feature/channel axis.\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    # Apply the network to the encoded features in memory-safe chunks along the batch dimension.\n",
    "    # This prevents out-of-memory errors when the total number of samples is very large.\n",
    "    # Ensure inputs are on the same device as the model parameters.\n",
    "    model_device = next(fn.parameters()).device\n",
    "    embedded = embedded.to(model_device)\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "\n",
    "    # Restore the original leading shape (e.g., [N_rays, N_samples]) and append the output channels.\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf79950",
   "metadata": {},
   "source": [
    "## Instantiate NeRF\n",
    "This section creates a function to instantiate NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab3b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf import NeRF\n",
    "\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\n",
    "    \"\"\"\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    model = NeRF(D=args.netdepth, W=args.netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                netchunk=args.netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce81b35",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f36d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    End-to-end NeRF training loop.\n",
    "\n",
    "    High-level overview for newcomers:\n",
    "    - Load a dataset of posed images (e.g., LLFF/Blender/LINEMOD). Each image comes with a camera pose.\n",
    "    - Create a NeRF model (coarse and optionally fine MLPs) and a renderer.\n",
    "    - On each iteration, sample camera rays and their target RGB values from the dataset.\n",
    "    - Render rays with the NeRF model via volumetric rendering (accumulate colors along the ray).\n",
    "    - Compute a reconstruction loss (e.g., MSE) against ground-truth pixels and optimize the networks.\n",
    "    - Periodically render validation trajectories and/or save snapshots.\n",
    "\n",
    "    Key concepts:\n",
    "    - Rays: For each pixel, we cast a ray into the scene with origin/direction computed from intrinsics and pose.\n",
    "    - Sampling: We sample multiple points along each ray (coarse). Optionally resample (fine) where the scene is likely.\n",
    "    - Volume rendering: Convert per-point density+color to opacity weights and composite to a final pixel color.\n",
    "    - Hierarchical sampling: A second pass focuses samples where the coarse pass is confident the scene exists.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------\n",
    "    # 1) Load data and choose near/far bounds depending on dataset\n",
    "    # ----------------------\n",
    "    K = None\n",
    "    if args.dataset_type == 'llff':\n",
    "        images, poses, bds, render_poses, i_test = load_llff_data(\n",
    "            args.datadir, args.factor, recenter=True, bd_factor=.75, spherify=args.spherify\n",
    "        )\n",
    "        hwf = poses[0,:3,-1]           # (H, W, focal)\n",
    "        poses = poses[:,:3,:4]         # Only keep rotation+translation (3x4) per pose\n",
    "        print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        if not isinstance(i_test, list):\n",
    "            i_test = [i_test]\n",
    "\n",
    "        # Optional LLFF holdout: use every N-th image as test\n",
    "        if args.llffhold > 0:\n",
    "            print('Auto LLFF holdout,', args.llffhold)\n",
    "            i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "\n",
    "        i_val = i_test\n",
    "        i_train = np.array([i for i in np.arange(int(images.shape[0]))\n",
    "                            if (i not in i_test and i not in i_val)])\n",
    "\n",
    "        print('DEFINING BOUNDS')\n",
    "        if args.no_ndc:\n",
    "            # If not using NDC (e.g., inward-facing/360 scenes), near/far from bounds\n",
    "            near = np.ndarray.min(bds) * .9\n",
    "            far = np.ndarray.max(bds) * 1.\n",
    "        else:\n",
    "            # Forward-facing (LLFF) uses NDC, so near/far are normalized\n",
    "            near = 0.\n",
    "            far = 1.\n",
    "        print('NEAR FAR', near, far)\n",
    "\n",
    "    elif args.dataset_type == 'blender':\n",
    "        images, poses, render_poses, hwf, i_split = load_blender_data(\n",
    "            args.datadir, args.half_res, args.testskip\n",
    "        )\n",
    "        print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        # Standard near/far for Blender synthetic scenes\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        # Composite over white if requested (makes background white instead of black)\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "    elif args.dataset_type == 'LINEMOD':\n",
    "        images, poses, render_poses, hwf, K, i_split, near, far = load_LINEMOD_data(\n",
    "            args.datadir, args.half_res, args.testskip\n",
    "        )\n",
    "        print(f'Loaded LINEMOD, images shape: {images.shape}, hwf: {hwf}, K: {K}')\n",
    "        print(f'[CHECK HERE] near: {near}, far: {far}.')\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "    elif args.dataset_type == 'deepvoxels':\n",
    "        images, poses, render_poses, hwf, i_split = load_dv_data(\n",
    "            scene=args.shape, basedir=args.datadir, testskip=args.testskip\n",
    "        )\n",
    "        print('Loaded deepvoxels', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        # DeepVoxels scenes define a hemisphere radius; near/far around it\n",
    "        hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))\n",
    "        near = hemi_R-1.\n",
    "        far = hemi_R+1.\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    # ----------------------\n",
    "    # 2) Prepare intrinsics (H, W, focal) and default K if not provided\n",
    "    # ----------------------\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if K is None:\n",
    "        # Construct a pinhole intrinsics matrix assuming principal point at image center\n",
    "        K = np.array([\n",
    "            [focal, 0, 0.5*W],\n",
    "            [0, focal, 0.5*H],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    # If we are evaluating on the test set, use the corresponding subset of poses\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    # ----------------------\n",
    "    # 3) Logging setup and persistence of configs\n",
    "    # ----------------------\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "\n",
    "    # Save the parsed args for reproducibility\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg, attr in sorted(args.items()):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    # Save the YAML configuration that produced these args\n",
    "    f_yaml = os.path.join(basedir, expname, 'config.yaml')\n",
    "    try:\n",
    "        utils.save_yaml(dict(args), f_yaml)\n",
    "    except Exception:\n",
    "        # Fallback: write a simple YAML dump directly\n",
    "        with open(f_yaml, 'w', encoding='utf-8') as yf:\n",
    "            import yaml as _yaml\n",
    "            _yaml.dump(dict(args), yf, default_flow_style=False, indent=2)\n",
    "\n",
    "    # ----------------------\n",
    "    # 4) Create NeRF models and optimizer\n",
    "    # ----------------------\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n",
    "    global_step = start\n",
    "\n",
    "    # Near/far bounds are used by the renderer; update both train and test configs\n",
    "    bds_dict = { 'near' : near, 'far' : far }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move the camera trajectory used for rendering validation videos to the GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "\n",
    "    # ----------------------\n",
    "    # 5) Short-circuit: render only mode\n",
    "    # ----------------------\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if args.render_test:\n",
    "                # Switch to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(\n",
    "                basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start)\n",
    "            )\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "\n",
    "            # Render a path and save to video\n",
    "            rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test,\n",
    "                                   gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "            print('Done rendering', testsavedir)\n",
    "            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "            return\n",
    "\n",
    "    # ----------------------\n",
    "    # 6) Prepare random-ray batching (optional) and move data to GPU\n",
    "    # ----------------------\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        # Precompute all rays for all training images, then shuffle mini-batches each step\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0)  # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1)                   # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4])                         # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0)                 # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3])                              # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move arrays/tensors to GPU for training\n",
    "    if use_batching:\n",
    "        images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)\n",
    "\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    # ----------------------\n",
    "    # 7) Main optimization loop\n",
    "    # ----------------------\n",
    "    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))  # Optional TB logging\n",
    "\n",
    "    start = start + 1\n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time()\n",
    "\n",
    "        # Sample a batch of rays and target colors\n",
    "        if use_batching:\n",
    "            # Random over all images (global batching)\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand]  # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            # Move sliding window; reshuffle at epoch end\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "        else:\n",
    "            # Random rays from a randomly chosen training image (per-image batching)\n",
    "            img_i = np.random.choice(i_train)\n",
    "            target = images[img_i]\n",
    "            target = torch.Tensor(target).to(device)\n",
    "            pose = poses[img_i, :3,:4]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                # Compute per-pixel rays for this image, then sample N_rand of them\n",
    "                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n",
    "\n",
    "                if i < args.precrop_iters:\n",
    "                    # Optional: focus early training on the image center (stabilizes training)\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n",
    "                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n",
    "                        ), -1)\n",
    "                    if i == start:\n",
    "                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n",
    "                else:\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1\n",
    "                    )  # (H, W, 2)\n",
    "\n",
    "                coords = torch.reshape(coords, [-1,2])                         # (H*W, 2)\n",
    "                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n",
    "                select_coords = coords[select_inds].long()                     # (N_rand, 2)\n",
    "                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]      # (N_rand, 3)\n",
    "                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]      # (N_rand, 3)\n",
    "                batch_rays = torch.stack([rays_o, rays_d], 0)\n",
    "                target_s = target[select_coords[:, 0], select_coords[:, 1]]    # (N_rand, 3)\n",
    "\n",
    "        # ---- Core rendering + loss ----\n",
    "        rgb, disp, acc, extras = render(\n",
    "            H, W, K, chunk=args.chunk, rays=batch_rays, verbose=i < 10, retraw=True, **render_kwargs_train\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_loss = img2mse(rgb, target_s)\n",
    "        trans = extras['raw'][...,-1]\n",
    "        loss = img_loss\n",
    "        psnr = mse2psnr(img_loss)\n",
    "\n",
    "        # If hierarchical sampling is enabled, include the coarse-pass loss\n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Learning rate decay (exponential) ---\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = args.lrate_decay * 1000\n",
    "        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "\n",
    "        dt = time.time()-time0\n",
    "        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 8) Periodic logging, checkpointing, and visualization\n",
    "        # ----------------------\n",
    "        if i%args.i_weights==0:\n",
    "            # Save model checkpoints for resuming or analysis\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            torch.save({\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i%args.i_video==0 and i > 0:\n",
    "            # Render a validation trajectory and write MP4 previews\n",
    "            with torch.no_grad():\n",
    "                rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            # If you want to visualize view-dependent effects, you can fix the camera position\n",
    "            # and vary only the view direction (see commented example in original code).\n",
    "\n",
    "        if i%args.i_testset==0 and i > 0:\n",
    "            # Render the held-out test set and save frames\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk,\n",
    "                            render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "        if i%args.i_print==0:\n",
    "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "            # Additional TensorBoard logging code is kept in comments in the original.\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c482509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert training parameters\n",
    "N_iters = 500 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f87ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image data (378, 504, 3, 20) [378.         504.         407.56579161]\n",
      "Loaded ./data/nerf_llff_data/fern 16.985296178676084 80.00209740336334\n",
      "recentered (3, 5)\n",
      "[[ 1.0000000e+00  0.0000000e+00  0.0000000e+00 -2.9802323e-09]\n",
      " [ 0.0000000e+00  1.0000000e+00 -1.8730975e-09 -9.6857544e-09]\n",
      " [-0.0000000e+00  1.8730975e-09  1.0000000e+00 -7.4505807e-10]]\n",
      "Data:\n",
      "(20, 3, 5) (20, 378, 504, 3) (20, 2)\n",
      "HOLDOUT view is 12\n",
      "Loaded llff (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.0 1.0\n",
      "Found ckpts []\n",
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n",
      "Begin\n",
      "TRAIN views are [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "TEST views are [ 0  8 16]\n",
      "VAL views are [ 0  8 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 99/500 [00:09<00:35, 11.18it/s]d:\\GitHub\\nerf-projects\\nerf\\.venv\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.001748800277709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([378, 504, 3]) torch.Size([378, 504])\n",
      "1 6.715870380401611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 99/500 [00:20<00:35, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6.541210174560547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6.305776834487915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6.63847017288208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6.396185398101807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6.288938283920288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 6.278231859207153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 6.339763164520264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 6.294673919677734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6.616713047027588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6.682886838912964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 6.634235382080078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 6.792020320892334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 6.570688247680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 6.402283191680908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 6.277316331863403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 6.273441314697266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 6.148571014404297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 6.256883382797241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 6.190611124038696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 6.296128511428833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 6.241514682769775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 6.262598037719727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 6.740360260009766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 6.583275318145752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 6.253154516220093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 6.199183225631714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 6.151266813278198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 6.142489671707153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 6.106160640716553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 6.036342620849609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 6.111091613769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 6.233784437179565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 6.15547513961792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 6.5495285987854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 6.501405954360962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 6.511346101760864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 6.515177488327026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 6.886274814605713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 6.917149543762207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 6.901323318481445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 6.183125257492065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 5.824687719345093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 5.861987829208374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 6.294087886810303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 6.719158887863159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 6.606377124786377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 6.401099681854248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 6.276803255081177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 6.136355876922607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 6.170569896697998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 6.191068410873413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 6.789647817611694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 6.767533779144287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 6.552538156509399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 6.620954751968384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 6.669760704040527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 6.748702526092529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 6.305450916290283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 6.719574928283691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 6.689786911010742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 6.698573350906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 6.490202903747559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 6.50455117225647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 6.726695537567139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 6.747149705886841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 6.688725233078003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 6.702297925949097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 6.7693212032318115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 6.744460582733154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 6.742389678955078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 6.753386974334717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 6.817801237106323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 6.781258821487427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 6.751497268676758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 6.859052658081055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 6.810867547988892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 6.7886574268341064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 6.801774024963379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 6.780665636062622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 6.791128158569336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 6.789022207260132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 6.813372850418091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 6.74838924407959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 6.5481486320495605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 6.450110912322998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 6.4858927726745605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 7.090197324752808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 6.831069469451904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 6.45445442199707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 6.508692264556885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 6.736927509307861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 6.855707406997681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 6.641560792922974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 6.817961931228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 6.799634218215942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 6.6927735805511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 6.755694627761841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6.783779859542847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 6.857285022735596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 6.767531633377075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 6.793948173522949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 6.72652530670166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 6.591036081314087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 6.512755393981934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 6.8005616664886475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 6.723774194717407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 6.702930212020874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 6.797089099884033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 6.785475015640259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 6.717003107070923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 6.765689134597778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 6.631833076477051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 6.653777837753296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 6.626197099685669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 6.658834457397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 6.794697523117065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 6.815761089324951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 6.766657114028931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [13:08<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, saving (120, 378, 504, 3) (120, 378, 504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (504, 378) to (512, 384) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (504, 378) to (512, 384) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      " 20%|██        | 102/500 [13:18<10:14:56, 92.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 100 Loss: 0.05030930042266846  PSNR: 15.956880569458008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 198/500 [13:27<00:25, 11.93it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0014901161193847656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([378, 504, 3]) torch.Size([378, 504])\n",
      "1 6.408746719360352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [00:07<13:59,  7.05s/it]\n",
      " 40%|███▉      | 199/500 [13:34<20:32,  4.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 320\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i%args.i_video==\u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i > \u001b[32m0\u001b[39m:\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# Render a validation trajectory and write MP4 previews\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         rgbs, disps = \u001b[43mrender_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_poses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhwf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_kwargs_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mDone, saving\u001b[39m\u001b[33m'\u001b[39m, rgbs.shape, disps.shape)\n\u001b[32m    322\u001b[39m     moviebase = os.path.join(basedir, expname, \u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_spiral_\u001b[39m\u001b[38;5;132;01m{:06d}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m.format(expname, i))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrender_path\u001b[39m\u001b[34m(render_poses, hwf, K, chunk, render_kwargs, gt_imgs, savedir, render_factor)\u001b[39m\n\u001b[32m     38\u001b[39m t = time.time()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Render the current pose; we discard the accumulated opacity and extras here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m rgb, disp, acc, _ = \u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2w\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc2w\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrender_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m rgbs.append(rgb.cpu().numpy())\n\u001b[32m     43\u001b[39m disps.append(disp.cpu().numpy())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(H, W, K, chunk, rays, c2w, ndc, near, far, use_viewdirs, c2w_staticcam, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     rays = torch.cat([rays, viewdirs], -\u001b[32m1\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Render all rays in memory-friendly chunks, then reshape results back to image grids.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m all_ret = \u001b[43mbatchify_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_ret:\n\u001b[32m     77\u001b[39m     k_sh = \u001b[38;5;28mlist\u001b[39m(sh[:-\u001b[32m1\u001b[39m]) + \u001b[38;5;28mlist\u001b[39m(all_ret[k].shape[\u001b[32m1\u001b[39m:])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mbatchify_rays\u001b[39m\u001b[34m(rays_flat, chunk, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Iterate over rays in chunks: [0:chunk], [chunk:2*chunk], ...\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, rays_flat.shape[\u001b[32m0\u001b[39m], chunk):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Render a minibatch of rays using the provided rendering function and settings.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     ret = \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# For each output field produced by the renderer, append the minibatch result\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# to a growing list so we can concatenate later.\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ret:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mrender_rays\u001b[39m\u001b[34m(ray_batch, network_fn, network_query_fn, N_samples, retraw, lindisp, perturb, N_importance, network_fine, white_bkgd, raw_noise_std, verbose, pytest)\u001b[39m\n\u001b[32m    113\u001b[39m     run_fn = network_fn \u001b[38;5;28;01mif\u001b[39;00m network_fine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m network_fine\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# raw = run_network(pts, fn=run_fn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     raw = \u001b[43mnetwork_query_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n\u001b[32m    119\u001b[39m ret = {\u001b[33m'\u001b[39m\u001b[33mrgb_map\u001b[39m\u001b[33m'\u001b[39m : rgb_map, \u001b[33m'\u001b[39m\u001b[33mdisp_map\u001b[39m\u001b[33m'\u001b[39m : disp_map, \u001b[33m'\u001b[39m\u001b[33macc_map\u001b[39m\u001b[33m'\u001b[39m : acc_map}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mcreate_nerf.<locals>.<lambda>\u001b[39m\u001b[34m(inputs, viewdirs, network_fn)\u001b[39m\n\u001b[32m     21\u001b[39m     model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n\u001b[32m     22\u001b[39m                       input_ch=input_ch, output_ch=output_ch, skips=skips,\n\u001b[32m     23\u001b[39m                       input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n\u001b[32m     24\u001b[39m     grad_vars += \u001b[38;5;28mlist\u001b[39m(model_fine.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m network_query_fn = \u001b[38;5;28;01mlambda\u001b[39;00m inputs, viewdirs, network_fn : \u001b[43mrun_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43membed_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43membeddirs_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddirs_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43mnetchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Create optimizer\u001b[39;00m\n\u001b[32m     32\u001b[39m optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.999\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrun_network\u001b[39m\u001b[34m(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk)\u001b[39m\n\u001b[32m     73\u001b[39m model_device = \u001b[38;5;28mnext\u001b[39m(fn.parameters()).device\n\u001b[32m     74\u001b[39m embedded = embedded.to(model_device)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m outputs_flat = \u001b[43mbatchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Restore the original leading shape (e.g., [N_rays, N_samples]) and append the output channels.\u001b[39;00m\n\u001b[32m     78\u001b[39m outputs = torch.reshape(outputs_flat, \u001b[38;5;28mlist\u001b[39m(inputs.shape[:-\u001b[32m1\u001b[39m]) + [outputs_flat.shape[-\u001b[32m1\u001b[39m]])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mbatchify.<locals>.ret\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mret\u001b[39m(inputs):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, inputs.shape[\u001b[32m0\u001b[39m], chunk)], \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\nerf-projects\\nerf\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\nerf-projects\\nerf\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\nerf-projects\\nerf\\nerf.py:74\u001b[39m, in \u001b[36mNeRF.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     72\u001b[39m h = \u001b[38;5;28mself\u001b[39m.pts_linears[i](h)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Apply ReLU activation function (makes negative values 0, keeps positive values)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m h = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# If this is a skip connection layer, concatenate the original input\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Skip connections help with gradient flow and learning deep networks\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# They're like \"shortcuts\" that let information flow directly from input to later layers\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skips:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\nerf-projects\\nerf\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1701\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1699\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1701\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c989e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
