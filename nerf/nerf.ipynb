{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6135c0",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields\n",
    "\n",
    "This notebook was written while referencing the original NeRF code so as to visualize step by step on how NeRF works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc42dfb",
   "metadata": {},
   "source": [
    "## Import Dependencies and Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c328110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third party libraries\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "DEBUG = False\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82faedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeted yaml folder: yaml\\llff_fern\n",
      "Loading configuration from yaml\\llff_fern\n",
      "Configuration validation passed! Arguments are valid and correctly set.\n",
      "Experiment name: nerf_experiment\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "# Load experiment config from yaml\n",
    "EXPERIMENT_NAME = \"llff_fern\"\n",
    "folder_path = os.path.join(\"yaml\", EXPERIMENT_NAME)\n",
    "print(f\"Targeted yaml folder: {folder_path}\")\n",
    "\n",
    "# Load the yaml data for use later in terms of args.\n",
    "args = utils.load_or_create_config(folder_path)\n",
    "print(f\"Experiment name: {args[\"expname\"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa389515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch import batchify\n",
    "\"\"\"\n",
    "Prepare 3D sample points for a NeRF-style network by applying positional encodings,\n",
    "run the network on these encodings in memory-safe chunks, and then reshape the results\n",
    "back to the original sampling layout.\n",
    "\n",
    "High-level intuition:\n",
    "- We often sample many 3D points (xyz) per ray and, optionally, use a per-ray viewing\n",
    "  direction. Raw coordinates are hard for small MLPs to learn high-frequency detail,\n",
    "  so we first apply a positional encoding that maps them to a higher-dimensional space.\n",
    "- We flatten everything to a big batch so the network can process all samples uniformly.\n",
    "- To avoid running out of memory, we split this big batch into chunks and process them\n",
    "  sequentially, then stitch the outputs back together and restore the original shape.\n",
    "\"\"\"\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"\n",
    "    Prepare inputs for a NeRF-style MLP and apply the network in chunks.\n",
    "\n",
    "    Conceptual overview:\n",
    "    - Positions (xyz) and, optionally, viewing directions are first positional-encoded\n",
    "      (a deterministic mapping to a higher-dimensional space using sin/cos at multiple\n",
    "      frequencies). This helps the MLP represent fine details and sharp changes.\n",
    "    - We flatten leading dimensions so all samples are processed as a single batch.\n",
    "    - To keep memory usage in check, we process this batch in chunks (netchunk).\n",
    "    - Finally, we reshape outputs to match the original sampling layout.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Sample positions with shape [..., Cpos], typically Cpos = 3.\n",
    "            Example: [N_rays, N_samples, 3]. The leading dimensions can be any shape.\n",
    "        viewdirs (Optional[torch.Tensor]): Per-ray viewing directions with shape\n",
    "            [N_rays, Cdir] (typically Cdir = 3), or None if not using view-dependent effects.\n",
    "            When provided, each ray direction is broadcast to all samples along that ray.\n",
    "        fn (Callable[[torch.Tensor], torch.Tensor]): Neural network (e.g., NeRF MLP) that\n",
    "            consumes encoded features and returns outputs per sample.\n",
    "        embed_fn (Callable[[torch.Tensor], torch.Tensor]): Positional encoder for positions;\n",
    "            maps [*, Cpos] -> [*, Cpos_enc].\n",
    "        embeddirs_fn (Optional[Callable[[torch.Tensor], torch.Tensor]]): Positional encoder\n",
    "            for directions; maps [*, Cdir] -> [*, Cdir_enc]. Only used if viewdirs is not None.\n",
    "        netchunk (int): Maximum number of samples to process per chunk to limit peak memory.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Network outputs with shape [..., Cout], where the leading dimensions\n",
    "        match those of `inputs` (excluding its last channel), and Cout is determined by `fn`.\n",
    "    \"\"\"\n",
    "    # Flatten all leading dimensions so we have a simple [N, Cpos] batch of positions.\n",
    "    # N is the total number of samples across rays and per-ray samples.\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "\n",
    "    # Positional-encode the flattened positions (e.g., apply sin/cos at multiple frequencies).\n",
    "    # This expands each 3D input into a richer, higher-dimensional representation\n",
    "    # that makes it easier for the MLP to model fine spatial detail.\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    # If using view-dependent appearance (e.g., specular highlights that vary with direction),\n",
    "    # we also encode per-ray viewing directions and concatenate them with position encodings.\n",
    "    if viewdirs is not None:\n",
    "        # Insert a length-1 axis, then broadcast each ray direction across all samples on that ray\n",
    "        # so that every sample point along a ray shares the same view direction.\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape)\n",
    "\n",
    "        # Flatten directions to align with the flattened positions: [N, Cdir].\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "\n",
    "        # Positional-encode viewing directions in the same spirit as positions.\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "\n",
    "        # Concatenate encoded positions and encoded directions along the feature/channel axis.\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    # Apply the network to the encoded features in memory-safe chunks along the batch dimension.\n",
    "    # This prevents out-of-memory errors when the total number of samples is very large.\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "\n",
    "    # Restore the original leading shape (e.g., [N_rays, N_samples]) and append the output channels.\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf79950",
   "metadata": {},
   "source": [
    "## Instantiate NeRF\n",
    "This section creates a function to instantiate NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import get_embedder\n",
    "from nerf import NeRF\n",
    "\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\n",
    "    \"\"\"\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    model = NeRF(D=args.netdepth, W=args.netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                netchunk=args.netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36d9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
